Real World Example 1: The Missing Revenue

Bug Report:
"Finance dashboard shows $2M less revenue than yesterday
 but no pipeline failed"

Your Data Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Postgres â”‚â”€â”€â”€â†’â”‚ Airflow  â”‚â”€â”€â”€â†’â”‚ Snowflakeâ”‚â”€â”€â”€â†’â”‚ Dashboard â”‚
â”‚ (Source) â”‚    â”‚ Pipeline â”‚    â”‚ (Warehouse)â”‚   â”‚ (Looker)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Step 1: Check pipeline status
  â†’ Airflow: All DAGs green âœ…
  â†’ No failures anywhere
  â†’ "But data is WRONG"

Step 2: Check dashboard query
  â†’ SQL looks correct âœ…
  â†’ Same query as last month

Step 3: Check Snowflake data
  â†’ SELECT SUM(revenue) FROM fact_orders 
     WHERE order_date = '2024-03-31';
  â†’ Result: $8M (yesterday it showed $10M for same date)
  â†’ 2M rows are MISSING

Step 4: Check Airflow extraction
  â†’ Extracted 500,000 rows today
  â†’ But source Postgres has 520,000 rows for that date
  â†’ 20,000 rows missing from extraction âŒ

Step 5: WHY are 20,000 rows missing?
  â†’ Check extraction query:
     SELECT * FROM orders 
     WHERE updated_at > '{{ last_run_time }}'
  â†’ last_run_time = 2024-03-31 00:00:00 UTC

Step 6: AHA MOMENT ğŸ’¡
  â†’ Daylight saving time changed last night
  â†’ Server timezone shifted
  â†’ last_run_time was calculated in LOCAL time
  â†’ But updated_at is stored in UTC
  â†’ 1 hour gap was created
  â†’ 20,000 orders fell into that gap
  â†’ They were NEVER extracted
  â†’ No error. Pipeline ran "successfully"

Step 7: Fix
  â†’ Force all timestamps to UTC
  â†’ Backfill the missing 1-hour window
  â†’ Add reconciliation check

Real World Example 2: The Duplicate Revenue

Bug Report:
"Revenue jumped 40% overnight. Sales team is celebrating.
 But it's probably wrong."

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka    â”‚â”€â”€â”€â†’â”‚ Spark    â”‚â”€â”€â”€â†’â”‚ Delta    â”‚
â”‚ (Events) â”‚    â”‚ Streamingâ”‚    â”‚ Lake     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Check if data is duplicated
  SELECT order_id, COUNT(*) 
  FROM fact_orders 
  GROUP BY order_id 
  HAVING COUNT(*) > 1;
  â†’ 100,000 duplicate order_ids âŒ

Step 2: When did duplicates start?
  SELECT DATE(loaded_at), COUNT(*) - COUNT(DISTINCT order_id) as dupes
  FROM fact_orders
  GROUP BY DATE(loaded_at)
  ORDER BY 1 DESC;
  â†’ Duplicates started 2 days ago

Step 3: What happened 2 days ago?
  â†’ Check deployment logs
  â†’ Spark streaming job was restarted
  â†’ Check Kafka consumer group offsets
  â†’ Consumer offset was RESET to 6 hours earlier
  â†’ 6 hours of events were reprocessed
  â†’ But Delta Lake merge had a bug

Step 4: Why did merge not catch duplicates?
  â†’ Check merge code:
     MERGE INTO fact_orders target
     USING staging source
     ON target.order_id = source.order_id
     WHEN MATCHED THEN UPDATE
     WHEN NOT MATCHED THEN INSERT
  
  â†’ Looks correct... BUT
  â†’ Staging table had duplicates WITHIN the same batch
  â†’ MERGE only deduplicates against TARGET
  â†’ It does NOT deduplicate within SOURCE
  â†’ So duplicate source rows both became "NOT MATCHED"
  â†’ Both got INSERTED

Step 5: Root cause chain
  Kafka offset reset
    â†’ Reprocessed 6 hours of events
      â†’ Staging had duplicate events
        â†’ MERGE didn't deduplicate within batch
          â†’ Duplicates inserted into Delta Lake
            â†’ Revenue inflated 40%
              â†’ Sales team celebrating fake numbers

Step 6: Fix
  â†’ Deduplicate staging BEFORE merge
  â†’ Add idempotency to streaming job
  â†’ Add duplicate detection alert
  â†’ Backfill correct data

Real World Example 3: The Slow Pipeline

Bug Report:
"Daily pipeline that usually takes 30 minutes 
 now takes 8 hours. Nothing changed in code."

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S3 Raw   â”‚â”€â”€â”€â†’â”‚ Spark on â”‚â”€â”€â”€â†’â”‚ S3 Clean â”‚
â”‚ (Parquet)â”‚    â”‚ EMR      â”‚    â”‚ (Parquet)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Check Spark UI
  â†’ One stage taking 7.5 hours
  â†’ 199 tasks finish in 2 minutes
  â†’ 1 task takes 7.5 hours â† DATA SKEW

Step 2: Which key is skewed?
  SELECT customer_id, COUNT(*) as cnt
  FROM raw_data
  GROUP BY customer_id
  ORDER BY cnt DESC
  LIMIT 10;
  
  â†’ customer_id = 'SYSTEM' has 50 MILLION rows
  â†’ All other customers have < 10,000 rows
  â†’ One partition has 50M rows, others have 10K

Step 3: WHY does 'SYSTEM' have 50M rows?
  â†’ Check with upstream team
  â†’ They deployed a new logging feature
  â†’ Every automated process now creates orders under 'SYSTEM'
  â†’ Data volume for that key grew 1000x overnight
  â†’ Nobody told the data team

Step 4: Fix
  â†’ Filter out 'SYSTEM' records (they're not real orders)
  â†’ OR salt the key to distribute across partitions
  â†’ OR handle 'SYSTEM' in a separate pipeline
  â†’ Add data volume monitoring alerts

Root cause chain:
  Upstream team deployed new feature
    â†’ 'SYSTEM' customer got 50M rows
      â†’ Spark join/groupby on customer_id
        â†’ One partition got all 50M rows
          â†’ One executor ran out of memory
            â†’ Spilled to disk
              â†’ 30 min pipeline â†’ 8 hours

Real World Example 4: The Silent Schema Change


Bug Report:
"Customer segmentation is completely wrong this week.
 High-value customers are showing as low-value."

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vendor   â”‚â”€â”€â”€â†’â”‚ Python   â”‚â”€â”€â”€â†’â”‚ BigQuery â”‚
â”‚ CSV/API  â”‚    â”‚ Ingestionâ”‚    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Step 1: Check segmentation logic
  â†’ SQL is correct âœ…
  â†’ Same query as always

Step 2: Check underlying data
  SELECT customer_id, lifetime_value 
  FROM dim_customer 
  ORDER BY lifetime_value DESC 
  LIMIT 10;
  â†’ Top customer shows $150 lifetime value
  â†’ That customer usually shows $150,000
  â†’ Values are 1000x smaller âŒ

Step 3: Check source data
  â†’ Download latest vendor CSV
  â†’ Column "revenue" now shows 150.00
  â†’ Last week it showed 150000.00
  â†’ VENDOR CHANGED FROM CENTS TO DOLLARS
  â†’ No notification
  â†’ No documentation update

Step 4: How deep is the damage?
  â†’ This column feeds into:
     â†’ dim_customer
     â†’ fact_transactions  
     â†’ 12 downstream dbt models
     â†’ 5 dashboards
     â†’ 1 ML model for churn prediction
     â†’ Finance monthly report (ALREADY SENT) ğŸ˜±

Step 5: Fix
  â†’ Add unit conversion logic
  â†’ Backfill affected date range
  â†’ Rebuild all downstream models
  â†’ Notify finance about incorrect report
  â†’ Add data drift detection
  â†’ Add vendor schema monitoring

Root cause chain:
  Vendor silently changed units
    â†’ Ingestion loaded wrong values
      â†’ No validation caught it
        â†’ Wrong values propagated everywhere
          â†’ Business decisions made on wrong data


1. Something LOOKS wrong in the data
2. Each individual system looks fine
3. The bug exists in the INTERACTION between systems
4. Root cause is in a system you DIDN'T expect
5. Finding it requires:
   â†’ Accessing multiple live systems
   â†’ Querying real data
   â†’ Talking to other teams
   â†’ Correlating events across time
   â†’ Creative hypothesis formation
   â†’ Trial and error investigation
6. AI cannot do ANY of steps in #5

7. 
